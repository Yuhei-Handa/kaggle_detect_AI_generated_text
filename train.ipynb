{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "subprocess.run([\"kaggle\", \"competitions\", \"download\", \"-c\", \"llm-detect-ai-generated-text\"])\n",
    "\n",
    "with zipfile.ZipFile(\"llm-detect-ai-generated-text.zip\") as zf:\n",
    "    zf.extractall(\"dataset\")\n",
    "\n",
    "os.remove(\"./llm-detect-ai-generated-text.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_essays = pd.DataFrame(pd.read_csv(\"./dataset/train_essays.csv\"))\n",
    "train_prompts = pd.DataFrame(pd.read_csv(\"./dataset/train_prompts.csv\"))\n",
    "test_essays = pd.DataFrame(pd.read_csv(\"./dataset/test_essays.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaa bbb ccc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>3</td>\n",
       "      <td>Bbb ccc ddd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>4</td>\n",
       "      <td>CCC ddd eee.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id          text\n",
       "0  0000aaaa          2  Aaa bbb ccc.\n",
       "1  1111bbbb          3  Bbb ccc ddd.\n",
       "2  2222cccc          4  CCC ddd eee."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and built the first ModelT. Cars have played a major role in our every day lives since then. But now, people are starting to question if limiting car usage would be a good thing. To me, limiting the use of cars might be a good thing to do.In like matter of this, article, \"In German Suburb, Life Goes On Without Cars,\" by Elizabeth Rosenthal states, how automobiles are the linchpin of suburbs, where middle class families from either Shanghai or Chicago tend to make their homes. Experts say how this is a huge impediment to current efforts to reduce greenhouse gas emissions from tailpipe. Passenger cars are responsible for 12 percent of greenhouse gas emissions in Europe...and up to 50 percent in some carintensive areas in the United States. Cars are the main reason for the greenhouse gas emissions because of a lot of people driving them around all the time getting where they need to go. Article, \"Paris bans driving due to smog,\" by Robert Duffer says, how Paris, after days of nearrecord pollution, enforced a partial driving ban to clear the air of the global city. It also says, how on Monday, motorist with evennumbered license plates were ordered to leave their cars at home or be fined a 22euro fine 31. The same order would be applied to oddnumbered plates the following day. Cars are the reason for polluting entire cities like Paris. This shows how bad cars can be because, of all the pollution that they can cause to an entire city.Likewise, in the article, \"Carfree day is spinning into a big hit in Bogota,\" by Andrew Selsky says, how programs that's set to spread to other countries, millions of Columbians hiked, biked, skated, or took the bus to work during a carfree day, leaving streets of this capital city eerily devoid of traffic jams. It was the third straight year cars have been banned with only buses and taxis permitted for the Day Without Cars in the capital city of 7 million. People like the idea of having carfree days because, it allows them to lesson the pollution that cars put out of their exhaust from people driving all the time. The article also tells how parks and sports centers have bustled throughout the city uneven, pitted sidewalks have been replaced by broad, smooth sidewalks rushhour restrictions have dramatically cut traffic and new restaurants and upscale shopping districts have cropped up. Having no cars has been good for the country of Columbia because, it has aloud them to repair things that have needed repairs for a long time, traffic jams have gone down, and restaurants and shopping districts have popped up, all due to the fact of having less cars around.In conclusion, the use of less cars and having carfree days, have had a big impact on the environment of cities because, it is cutting down the air pollution that the cars have majorly polluted, it has aloud countries like Columbia to repair sidewalks, and cut down traffic jams. Limiting the use of cars would be a good thing for America. So we should limit the use of cars by maybe riding a bike, or maybe walking somewhere that isn't that far from you and doesn't need the use of a car to get you there. To me, limiting the use of cars might be a good thing to do.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# \"text\" 列の各行に対して \"\\n\\n\" を削除する関数を定義\n",
    "def remove_newline(text):\n",
    "    text = re.sub(r\"\\n\\n\", \"\", text)\n",
    "    return text.replace(\"\\n\\n\", \" \").replace(r\"\\\\n\\\\n\", \"\")\n",
    "\n",
    "# \"text\" 列に対して上記の関数を適用\n",
    "train_essays[\"text\"] = train_essays[\"text\"].apply(remove_newline)\n",
    "\n",
    "# 結果を表示\n",
    "print(train_essays[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'There are many advantages to limiting car usage in our community. Other countries such as France, Germany, and Colombia are home to cities that are working towards cutting down the use of personal automobiles. Many of the people in these places find that using alternative transportation means proves to be less stressful. Studies even show that fewer Americans are purchasing cars for themselves. Now is the perfect time to join in with Vauban, Bogota, and New York and spend less time in our cars.It may seem hard to believe, but in Germany, there\\'s a suburban area where residents live without their own cars. According to \"In German Suburb, Life Goes On Without Cars\" by Elisabeth Rosenthal, the streets of Vauban, Germany remain \"carfree\" aside from some public transport. The article states that \"70 percent of vaughn\\'s families do not own cars, and 57 percent sold a car to move here.\". Heidrun Walter was quoted in the excerpt saying \"When I had a car I was always tense. I\\'m much happier this way...\" Communities in Europe and the United States are hoping to move towards this \"carfree\" lifestyle, in order to become less dependent on automobile usage and cut back on greenhouse gas emissions that damage the environment. We may see an increase in the number of \"smart planning\" areas across the globe. If a mass of citizens wish to lessen the amount of time they spend in their cars, it\\'s possible, and the city can adapt to a more automobileless way of life.Bogota, Columbia dedicates a day to transportation without the use of personal cars, where the city\\'s goal is \"...to promote alternative transportation and reduce smog\". Many who visit Bogota during this time are impressed by the \"revolutionary change\" they see unfold before their eyes. Going \"carfree\" leads to more physical activity amongst residents and an overall nicerlooking community. \"Carfree day is spinning into a big hit in Bogota\" by Andrew Selsky claims \"Parks and sports centers... have bloomed throughout the city... sidewalks have been replaced by broad, smooth sidewalks... restaraunts and upscale shopping districts have cropped up.\". Not only does the city reduce the amount of greenhouse gas it contributes to the atmosphere, but it results in a more active and betterlooking community.What change would we see in our own community were we to follow in the footsteps of Bogota and Vauban? The United States is seeing a decrease of car ownership in the country. Less and less people are buying automoblies and obtaining driver\\'s licenses. \"The End of Car Culture\" by Elisabeth Rosenthal says that \"...America\\'s love affair with its vehicles seems to be cooling.\" The writer cites investment research company Doug Short of Advisor Perspectives, which states \"...the number of miles driven in the United States peaked in 2005 and dropped steadily thereafter...\" The country is already subconsciously moving towards a more carindependent lifestyle. The millenial generation seems to be the biggest contributor to this declined interest in carownership. With improved methods of communication by means of social media and cellphones, as well as more use of carpooling and public transportation, people are staving away from car commuting. While this may require a change within the automobile industry, many agree that this turning away from private car usage will see communities striving to be more time and energy efficient when it comes to transportation.Limiting the use of cars can lead to a less polluted and stressful environment, more exerciseoriented and upscale communities, and the conservation of our natural resources. Cities around the world are working to become less cardependent, so that they may limit their contribution of greenhouse gases to the atmosphere. Were our community to move towards this more ecofriendly, carfree way of life, we would not be alone.', 'generated': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "train_essays_dataset =  Dataset.from_pandas(train_essays[[\"text\", \"generated\"]])\n",
    "dataset_split_test = train_essays_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "print(dataset_split_test[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n",
      "bos_token : <s> , 0\n",
      "eos_token : </s> , 2\n",
      "unk_token : <unk> , 3\n",
      "pad_token : <pad> , 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(\"bos_token :\", tokenizer.bos_token, \",\", tokenizer.bos_token_id)\n",
    "print(\"eos_token :\", tokenizer.eos_token, \",\", tokenizer.eos_token_id)\n",
    "print(\"unk_token :\", tokenizer.unk_token, \",\", tokenizer.unk_token_id)\n",
    "print(\"pad_token :\", tokenizer.pad_token, \",\", tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1339392 || all params: 126036825 || trainable%: 1.0626989373938927\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "\n",
    "# トークナイザーとモデルの準備\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "masked_base_model = AutoModelForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "# PEFTのコンフィグを設定\n",
    "# PEFTの設定\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"key\",  \"value\", \"out_proj\", \"dense\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",  # テキスト分類の場合は\"SEQ_CLS\"となる\n",
    ")\n",
    "\n",
    "# PEFTモデルを作成\n",
    "masked_model = get_peft_model(masked_base_model, peft_config)\n",
    "print_trainable_parameters(masked_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset.iloc[idx]\n",
    "        labels = data[\"generated\"]\n",
    "        tokenized_texts = self.tokenizer(data[\"text\"], return_tensors='pt',truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "        return tokenized_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience):\n",
    "        self.check_count = 0\n",
    "        self.patience = patience\n",
    "\n",
    "    def checkCount(self, _bool):\n",
    "        if _bool:\n",
    "            self.check_count = 0\n",
    "        else:\n",
    "            self.check_count += 1\n",
    "\n",
    "        if self.check_count == self.patience:\n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "early_stopping = EarlyStopping(patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [16, 50265], got [16]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yuhei\\VScode\\python\\LLM_Detect_AI_Generated_Text\\train.ipynb セル 10\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/train.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m outputs \u001b[39m=\u001b[39m masked_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\u001b[39m.\u001b[39mlogits\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/train.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(softmax(outputs), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/train.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/train.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m correct \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(pred \u001b[39m==\u001b[39m labels) \u001b[39m/\u001b[39m changeable_batch_size\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/train.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [16, 50265], got [16]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaConfig\n",
    ")\n",
    "\"C:/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/dataset/\"\n",
    "train_dataframe = pd.DataFrame(pd.read_csv(\"C:/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/dataset/train_essays.csv\"))\n",
    "train_dataframe1 = pd.DataFrame(pd.read_csv(\"C:/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/dataset/Training_Essay_Data.csv\"))\n",
    "train_dataframe2 = pd.DataFrame(pd.read_csv(\"C:/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/dataset/Wikipedia.csv\"))\n",
    "train_dataframe3 = pd.DataFrame(pd.read_csv(\"C:/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/dataset/data_set.csv\"))\n",
    "train_dataframe4 = pd.DataFrame(pd.read_csv(\"C:/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/dataset/train_essays_RDizzl3_seven_v2.csv\"))\n",
    "train_dataframe5 = pd.DataFrame(pd.read_csv(\"C:/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/dataset/dataset.csv\"))\n",
    "\n",
    "tmp_train_dataframe = train_dataframe.drop(columns=[\"id\", \"prompt_id\"]).rename(columns={\"generated\": \"labels\"})\n",
    "tmp_train_dataframe.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "dataset_length = len(tmp_train_dataframe)\n",
    "\n",
    "tmp_test_dataframe = tmp_train_dataframe.iloc[int(dataset_length * 0.8):]\n",
    "tmp_train_dataframe = tmp_train_dataframe.iloc[:int(dataset_length * 0.8)]\n",
    "\n",
    "\n",
    "tmp_train_dataframe1 = train_dataframe1.rename(columns={\"generated\": \"labels\"})\n",
    "tmp_train_dataframe2 = train_dataframe2.drop(columns=[\"Human\"]).rename(columns={\"Text\":\"text\", \"AI\": \"labels\"})\n",
    "tmp_train_dataframe3 = train_dataframe3.drop(columns=[\"title\", \"ai_generated\"]).rename(columns={\"abstract\":\"text\", \"is_ai_generated\": \"labels\"})\n",
    "tmp_train_dataframe4 = train_dataframe4.rename(columns={\"label\": \"labels\"})\n",
    "tmp_train_dataframe5 = train_dataframe5.drop(columns=[\"Unnamed: 0\"]).rename(columns={\"class\": \"labels\"})\n",
    "tmp_train_dataframe5[\"labels\"] = tmp_train_dataframe5[\"labels\"].apply(lambda x: 1 if x == \"AI-Generated-Text\" else 0)\n",
    "\n",
    "def concat_dataframe(*dataframes):\n",
    "    result_dataframe = pd.concat(dataframes, axis=0)\n",
    "    return result_dataframe\n",
    "\n",
    "train_essays = concat_dataframe(\n",
    "    tmp_train_dataframe,\n",
    "    tmp_train_dataframe1,\n",
    "    tmp_train_dataframe4,\n",
    ")\n",
    "\n",
    "# 特定の文字を取り除く\n",
    "target_word = \"\\n\"\n",
    "train_essays['text'] =train_essays['text'].str.replace(target_word, '')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def max_token_count(dataframe, model_name=\"roberta-base\"):\n",
    "    # トークナイザーの読み込み\n",
    "    token_counts_list = []\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.model_max_length = 1027\n",
    "\n",
    "    # トークン数を計算してリストに保存\n",
    "\n",
    "    for text in tqdm(dataframe[\"text\"]):\n",
    "      token_counts_list.append(len(tokenizer.encode(text)))\n",
    "\n",
    "    return max(token_counts_list), token_counts_list\n",
    "\n",
    "max_seq_length, token_counts_list = max_token_count(train_essays)\n",
    "\n",
    "lower_max_length_list = [count <= 1024 for count in token_counts_list]\n",
    "\n",
    "filterd_dataframe = train_essays[lower_max_length_list]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=1024):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe['text'].iloc[idx]\n",
    "        label = self.dataframe['labels'].iloc[idx]\n",
    "\n",
    "        # Tokenize and encode the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Extract input_ids, attention_mask, and convert label to tensor\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        label_tensor = torch.tensor(label)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label_tensor\n",
    "        }\n",
    "    \n",
    "# データを訓練用と評価用に分割\n",
    "train_dataframe, eval_dataframe = train_test_split(filterd_dataframe, test_size=0.1, random_state=42)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer.model_max_length = 1027\n",
    "batch_size = 2\n",
    "gradient_accumulation_size = 64\n",
    "check_steps = gradient_accumulation_size / batch_size\n",
    "\n",
    "train_dataset = CustomDataset(train_dataframe, tokenizer)\n",
    "eval_dataset = CustomDataset(eval_dataframe, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "config = RobertaConfig(\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    hidden_size=768,\n",
    "    initializer_range=0.2,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=1027,\n",
    "    num_attention_heads=12,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    type_vocab_size=1,\n",
    "    pad_token_id=1,\n",
    "    num_hidden_layers=12,\n",
    "    output_attentions=False,\n",
    "    vocab_size=50265\n",
    "\n",
    ")\n",
    "\n",
    "model = RobertaForSequenceClassification(config)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience):\n",
    "        self.check_count = 0\n",
    "        self.patience = patience\n",
    "\n",
    "    def checkCount(self, _bool):\n",
    "        if _bool:\n",
    "            self.check_count = 0\n",
    "        else:\n",
    "            self.check_count += 1\n",
    "\n",
    "        if self.check_count == self.patience:\n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "patience = 5\n",
    "early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "total_samples = len(train_dataset)\n",
    "class_0_samples = len(train_dataframe[train_dataframe[\"labels\"] == 0])\n",
    "class_1_samples = len(train_dataframe[train_dataframe[\"labels\"] == 1])\n",
    "\n",
    "# クラスごとの重みを計算する。\n",
    "weight_class_0 = total_samples / class_0_samples\n",
    "weight_class_1 = total_samples / class_1_samples\n",
    "\n",
    "# 重みをテンソルに変換\n",
    "weights = torch.tensor([weight_class_0, weight_class_1]).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "model = model.to(device)\n",
    "\n",
    "def compute_metrics(label_ids, predictions):\n",
    "    # AUC-ROCの計算\n",
    "    probabilities = F.softmax(predictions, dim=-1).cpu().detach().numpy()\n",
    "    auc_roc = roc_auc_score(label_ids.cpu().detach().numpy(), probabilities[:, 1])  # 2クラス分類の場合、クラス1の確率を使います\n",
    "\n",
    "    return auc_roc\n",
    "\n",
    "def train(model, train_dataloader, optimizer, criterion, check_steps, device=\"cuda\"):\n",
    "    epoch_loss = 0.0\n",
    "    sum_accumulation_loss = 0.0\n",
    "\n",
    "    #epoch_auc_roc = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch, data in tqdm(enumerate(train_dataloader)):\n",
    "        input_ids =data[\"input_ids\"].to(device)\n",
    "        attention_mask = data[\"attention_mask\"].to(device)\n",
    "        labels =  data[\"labels\"].to(device)\n",
    "\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        accumulation_loss = loss / check_steps\n",
    "        accumulation_loss.backward()\n",
    "        sum_accumulation_loss += accumulation_loss.item()\n",
    "\n",
    "        #auc_roc = compute_metrics(labels, outputs.logits)\n",
    "\n",
    "        if (batch + 1) % check_steps == 0:\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print()\n",
    "            print(\"Gradient Accumulation\")\n",
    "            print(f\"Step: Train Iteration: {batch + 1}/{len(train_dataloader)} loss: {sum_accumulation_loss}\")\n",
    "            sum_accumulation_loss = 0.0\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        #epoch_auc_roc += auc_roc\n",
    "\n",
    "        print()\n",
    "        #print(f\"Step: Train Iteration: {batch + 1}/{len(train_dataloader)} loss: {loss.item()}\")\n",
    "        #print(f\"Step: Train Iteration: {batch + 1} loss: {loss.item()}, auc_roc: {auc_roc.item()}\")\n",
    "\n",
    "    return epoch_loss / (batch + 1)#, epoch_auc_roc / (batch + 1)\n",
    "\n",
    "def evaluate(model, eval_dataloader, criterion, device=\"cuda\"):\n",
    "    epoch_loss = 0.0\n",
    "    #epoch_auc_roc = 0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, data in tqdm(enumerate(eval_dataloader)):\n",
    "            input_ids =data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            labels =  data[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            #auc_roc = compute_metrics(outputs.logits, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            #epoch_auc_roc += auc_roc\n",
    "\n",
    "            print()\n",
    "            print(f\"Step: Eval Iteration: {batch + 1}/{len(eval_dataloader)} loss: {loss.item()}\")\n",
    "            #print(f\"Step: Eval Iteration: {batch + 1} loss: {loss.item()}, auc_roc: {auc_roc.item()}\")\n",
    "\n",
    "    return epoch_loss / (batch + 1)#, epoch_auc_roc / (batch + 1)\n",
    "\n",
    "train_loss_list = []\n",
    "#train_auc_roc_list = []\n",
    "\n",
    "eval_loss_list = []\n",
    "#eval_auc_roc_list = []\n",
    "\n",
    "epochs = 20\n",
    "min_loss = 1000\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_train_loss = train(model, train_dataloader, optimizer, criterion, check_steps)\n",
    "    print()\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Step: Train Epoch: {epoch + 1}/{epochs}, loss: {epoch_train_loss}\")\n",
    "    #print(f\"Step: Train Epoch: {epoch + 1}/{epochs}, loss: {epoch_train_loss}, auc_roc: {epoch_train_auc_roc}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    epoch_eval_loss = evaluate(model, eval_dataloader, criterion)\n",
    "    print()\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Step: Eval Epoch: {epoch + 1}/{epochs}, loss: {epoch_eval_loss}\")\n",
    "    #print(f\"Step: Eval Epoch: {epoch + 1}/{epochs}, loss: {epoch_eval_loss}, auc_roc: {epoch_eval_auc_roc}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    train_loss_list.append(epoch_train_loss)\n",
    "    #train_auc_roc_list.append(epoch_train_auc_roc)\n",
    "\n",
    "    eval_loss_list.append(epoch_eval_loss)\n",
    "    #eval_auc_roc_list.append(epoch_eval_auc_roc)\n",
    "\n",
    "    if epoch_eval_loss < min_loss:\n",
    "        min_loss = epoch_eval_loss\n",
    "        model.save_pretrained(\"./roberta_classification_model/\")\n",
    "        early_stopping.checkCount(True)\n",
    "\n",
    "    else:\n",
    "      check = early_stopping.checkCount(False)\n",
    "      if check == 0:\n",
    "        print(\"Early Stopping !\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2548260 || all params: 126603300 || trainable%: 2.0127911357760815\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import RobertaForSequenceClassification\n",
    "import torch\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "# ベースのRobertaモデルを読み込む\n",
    "base_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "# PEFTのコンフィグを設定\n",
    "# PEFTの設定\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"key\",  \"value\", \"out_proj\", \"dense\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",  # テキスト分類の場合は\"SEQ_CLS\"となる\n",
    ")\n",
    "\n",
    "# PEFTモデルを作成\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1378\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset.iloc[idx]\n",
    "        labels = data[\"generated\"]\n",
    "        tokenized_texts = self.tokenizer(data[\"text\"], return_tensors='pt',truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "        return tokenized_texts, labels\n",
    "    \n",
    "train_dataset = MyDataset(train_essays, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience):\n",
    "        self.check_count = 0\n",
    "        self.patience = patience\n",
    "\n",
    "    def checkCount(self, _bool):\n",
    "        if _bool:\n",
    "            self.check_count = 0\n",
    "        else:\n",
    "            self.check_count += 1\n",
    "\n",
    "        if self.check_count == self.patience:\n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "early_stopping = EarlyStopping(patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 1/69 train_loss: 0.7375463843345642 train_correct 0.25\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 2/69 train_loss: 0.7475179433822632 train_correct 0.125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 3/69 train_loss: 0.7467154661814371 train_correct 0.125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 4/69 train_loss: 0.7467857748270035 train_correct 0.125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 5/69 train_loss: 0.7478360772132874 train_correct 0.1\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 6/69 train_loss: 0.7466359833876292 train_correct 0.11458333333333333\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 7/69 train_loss: 0.7490234545298985 train_correct 0.11607142857142858\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 8/69 train_loss: 0.7491680681705475 train_correct 0.109375\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 9/69 train_loss: 0.748780263794793 train_correct 0.1111111111111111\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 10/69 train_loss: 0.7455938160419464 train_correct 0.13125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 11/69 train_loss: 0.7442778511480852 train_correct 0.125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 12/69 train_loss: 0.7445226162672043 train_correct 0.11979166666666667\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 13/69 train_loss: 0.7448685719416692 train_correct 0.125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 14/69 train_loss: 0.7446186287062508 train_correct 0.12053571428571429\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 15/69 train_loss: 0.7435227672259013 train_correct 0.12083333333333333\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 16/69 train_loss: 0.7455139197409153 train_correct 0.11328125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 17/69 train_loss: 0.7451767500709084 train_correct 0.11029411764705882\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 18/69 train_loss: 0.7446794112523397 train_correct 0.1111111111111111\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 19/69 train_loss: 0.7431696653366089 train_correct 0.11842105263157894\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 20/69 train_loss: 0.7428739428520202 train_correct 0.11875\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 21/69 train_loss: 0.7427698771158854 train_correct 0.11904761904761904\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 22/69 train_loss: 0.7434057186950337 train_correct 0.11647727272727272\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 23/69 train_loss: 0.7425355289293372 train_correct 0.11956521739130435\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 24/69 train_loss: 0.7433737640579542 train_correct 0.1171875\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 25/69 train_loss: 0.7436531591415405 train_correct 0.115\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 26/69 train_loss: 0.7435680834146646 train_correct 0.11298076923076923\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 27/69 train_loss: 0.743205624598044 train_correct 0.11574074074074074\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 28/69 train_loss: 0.7434193555797849 train_correct 0.11383928571428571\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 29/69 train_loss: 0.74392654361396 train_correct 0.11206896551724138\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 30/69 train_loss: 0.7438241561253865 train_correct 0.11041666666666666\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 31/69 train_loss: 0.7431996253228956 train_correct 0.11088709677419355\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 32/69 train_loss: 0.7422623634338379 train_correct 0.1171875\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 33/69 train_loss: 0.7418099930792144 train_correct 0.11931818181818182\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 34/69 train_loss: 0.7412906797493205 train_correct 0.11948529411764706\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 35/69 train_loss: 0.7386240039552961 train_correct 0.11964285714285715\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 36/69 train_loss: 0.7384448415703244 train_correct 0.11979166666666667\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 37/69 train_loss: 0.7378980871793386 train_correct 0.11824324324324324\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 38/69 train_loss: 0.7375186556263974 train_correct 0.12006578947368421\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 39/69 train_loss: 0.7374011965898367 train_correct 0.12179487179487179\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 40/69 train_loss: 0.7382441624999047 train_correct 0.1203125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 41/69 train_loss: 0.7384049470831708 train_correct 0.11890243902439024\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 42/69 train_loss: 0.7382766462507702 train_correct 0.12053571428571429\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 43/69 train_loss: 0.738309362600016 train_correct 0.11918604651162791\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 44/69 train_loss: 0.7382293059067293 train_correct 0.12215909090909091\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 45/69 train_loss: 0.7383192049132453 train_correct 0.12222222222222222\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 46/69 train_loss: 0.7379441766635232 train_correct 0.12364130434782608\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 47/69 train_loss: 0.7382197595657186 train_correct 0.12101063829787234\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 48/69 train_loss: 0.738038799415032 train_correct 0.12369791666666667\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 49/69 train_loss: 0.7382977677851307 train_correct 0.1211734693877551\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 50/69 train_loss: 0.7382645761966705 train_correct 0.12125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 51/69 train_loss: 0.7379081494667951 train_correct 0.12254901960784313\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 52/69 train_loss: 0.7372726339560288 train_correct 0.12620192307692307\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 53/69 train_loss: 0.7373457944618081 train_correct 0.125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 54/69 train_loss: 0.7370077857264766 train_correct 0.12731481481481483\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 55/69 train_loss: 0.7367058179595254 train_correct 0.12613636363636363\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 56/69 train_loss: 0.7367273909705025 train_correct 0.12611607142857142\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 57/69 train_loss: 0.7371035803828323 train_correct 0.125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 58/69 train_loss: 0.7369283355515579 train_correct 0.125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 59/69 train_loss: 0.7369840064291227 train_correct 0.1260593220338983\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 60/69 train_loss: 0.7364086230595907 train_correct 0.12708333333333333\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 61/69 train_loss: 0.7362141482165603 train_correct 0.12807377049180327\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 62/69 train_loss: 0.7360201487618108 train_correct 0.1280241935483871\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 63/69 train_loss: 0.7360519378904312 train_correct 0.12698412698412698\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 64/69 train_loss: 0.7362258592620492 train_correct 0.126953125\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 65/69 train_loss: 0.7360172134179336 train_correct 0.12692307692307692\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 66/69 train_loss: 0.7358300279487263 train_correct 0.1268939393939394\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 67/69 train_loss: 0.7355114185988013 train_correct 0.1287313432835821\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 68/69 train_loss: 0.7361007727244321 train_correct 0.12775735294117646\n",
      "Fold: 1 Step: Train Epoch: 1/5 Iteration: 69/69 train_loss: 0.7364391485850016 train_correct 0.12590579710144928\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 1/18 val_loss: 0.7413440942764282 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 2/18 val_loss: 0.7416013479232788 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 3/18 val_loss: 0.7420624693234762 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 4/18 val_loss: 0.7419915944337845 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 5/18 val_loss: 0.742074167728424 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 6/18 val_loss: 0.7417876323064169 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 7/18 val_loss: 0.7415834580148969 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 8/18 val_loss: 0.7416600361466408 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 9/18 val_loss: 0.7418555749787225 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 10/18 val_loss: 0.7417075753211975 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 11/18 val_loss: 0.7416976907036521 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 12/18 val_loss: 0.7417150437831879 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 13/18 val_loss: 0.7417908448439378 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 14/18 val_loss: 0.7418009553636823 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 15/18 val_loss: 0.7419763525327047 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 16/18 val_loss: 0.7419321425259113 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 17/18 val_loss: 0.7419333703377667 val_correct 0.0\n",
      "Fold: 1 Step: Val Epoch: 1/5 Iteration: 18/18 val_loss: 0.7367831336127387 val_correct 0.013888888888888888\n",
      "Model Save !\n",
      "Fold: 1 Step: Train Epoch: 2/5 Iteration: 1/69 train_loss: 0.756229817867279 train_correct 0.0625\n",
      "Fold: 1 Step: Train Epoch: 2/5 Iteration: 2/69 train_loss: 0.7442092597484589 train_correct 0.09375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yuhei\\VScode\\python\\LLM_Detect_AI_Generated_Text\\train.ipynb セル 11\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/train.ipynb#X13sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/train.ipynb#X13sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/train.ipynb#X13sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/train.ipynb#X13sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuhei/VScode/python/LLM_Detect_AI_Generated_Text/train.ipynb#X13sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = torch.tensor().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "output_dir = \"./output\"\n",
    "check = None\n",
    "min_loss = np.inf\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "splits = folds.split(np.zeros(len(train_essays)), train_essays[\"generated\"])\n",
    "\n",
    "train_loss_lists = []\n",
    "train_correct_lists = []\n",
    "\n",
    "val_loss_lists = []\n",
    "val_correct_lists = []\n",
    "\n",
    "\n",
    "\n",
    "for fold, (train_idxs, val_idxs) in enumerate(splits):\n",
    "    model = get_peft_model(base_model, peft_config).to(device)\n",
    "    train_dataset = MyDataset(train_essays.iloc[train_idxs], tokenizer)\n",
    "    val_dataset = MyDataset(train_essays.iloc[val_idxs], tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_loss_list = []\n",
    "    train_correct_list = []\n",
    "\n",
    "    val_loss_list = []\n",
    "    val_correct_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step in [\"train\", \"val\"]:\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0.0\n",
    "\n",
    "            if step == \"train\":\n",
    "                model.train()\n",
    "                dataloader = train_dataloader\n",
    "\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_dataloader\n",
    "\n",
    "            for batch, (inputs, labels) in enumerate(dataloader):\n",
    "                changeable_batch_size = len(labels)\n",
    "                inputs[\"input_ids\"] = inputs[\"input_ids\"].squeeze(1) # (batch_size, 1, seq_length) -> (batch_size, seq_length)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                with torch.set_grad_enabled(step == \"train\"):\n",
    "                    outputs = model(**inputs).logits\n",
    "                    pred = torch.argmax(softmax(outputs), dim=-1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    correct = torch.sum(pred == labels) / changeable_batch_size\n",
    "\n",
    "                    if step == \"train\":\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                running_correct += correct.item()\n",
    "\n",
    "                if step == \"train\":\n",
    "                    print(f\"Fold: {fold + 1} Step: Train Epoch: {epoch + 1}/{epochs} Iteration: {batch + 1}/{len(dataloader)} train_loss: {running_loss / (batch + 1)} train_correct {running_correct / (batch + 1)}\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Fold: {fold + 1} Step: Val Epoch: {epoch + 1}/{epochs} Iteration: {batch + 1}/{len(dataloader)} val_loss: {running_loss / (batch + 1)} val_correct {running_correct / (batch + 1)}\")\n",
    "            \n",
    "            if step == \"train\":\n",
    "                train_loss_list.append(running_loss / (batch + 1))\n",
    "                train_correct_list.append(running_correct / (batch + 1))\n",
    "            else:\n",
    "                val_loss_list.append(running_loss / (batch + 1))\n",
    "                val_correct_list.append(running_correct / (batch + 1))                \n",
    "\n",
    "                if running_loss < min_loss:\n",
    "                    early_stopping.checkCount(True)\n",
    "                    print(\"Model Save !\")\n",
    "                    min_loss = running_loss\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.mkdir(output_dir)\n",
    "                    torch.save(model.state_dict(), os.path.join(output_dir, \"model.pth\"))\n",
    "                else:\n",
    "                    check = early_stopping.checkCount(False)\n",
    "                if check == 0:\n",
    "                    print(\"Early Stopping !\") \n",
    "                    break\n",
    "    train_loss_lists.append(train_loss_list)\n",
    "    train_correct_lists.append(train_correct_list)\n",
    "    val_loss_lists.append(val_loss_list)\n",
    "    val_correct_lists.append(val_correct_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
